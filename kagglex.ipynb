{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install dependencies\n!pip install -q transformers accelerate bitsandbytes peft\n!pip install -q huggingface_hub","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:14.336331Z","iopub.execute_input":"2024-11-18T18:21:14.336665Z","iopub.status.idle":"2024-11-18T18:21:36.063478Z","shell.execute_reply.started":"2024-11-18T18:21:14.336629Z","shell.execute_reply":"2024-11-18T18:21:36.062494Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Download dataset directly\n!wget https://huggingface.co/datasets/Amod/mental_health_counseling_conversations/resolve/main/combined_dataset.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:36.065353Z","iopub.execute_input":"2024-11-18T18:21:36.065644Z","iopub.status.idle":"2024-11-18T18:21:37.511535Z","shell.execute_reply.started":"2024-11-18T18:21:36.065615Z","shell.execute_reply":"2024-11-18T18:21:37.510695Z"}},"outputs":[{"name":"stdout","text":"--2024-11-18 18:21:36--  https://huggingface.co/datasets/Amod/mental_health_counseling_conversations/resolve/main/combined_dataset.json\nResolving huggingface.co (huggingface.co)... 108.156.201.48, 108.156.201.111, 108.156.201.102, ...\nConnecting to huggingface.co (huggingface.co)|108.156.201.48|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4790520 (4.6M) [text/plain]\nSaving to: 'combined_dataset.json'\n\ncombined_dataset.js 100%[===================>]   4.57M  17.0MB/s    in 0.3s    \n\n2024-11-18 18:21:37 (17.0 MB/s) - 'combined_dataset.json' saved [4790520/4790520]\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Add at the start of your code\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Set environment variables\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"JAX_DISABLE_FORK\"] = \"1\"\n\n# Update torch amp settings\nimport torch\ntorch.amp.GradScaler = lambda *args, **kwargs: torch.amp.GradScaler(\"cuda\", *args, **kwargs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:37.512746Z","iopub.execute_input":"2024-11-18T18:21:37.513050Z","iopub.status.idle":"2024-11-18T18:21:40.283946Z","shell.execute_reply.started":"2024-11-18T18:21:37.513020Z","shell.execute_reply":"2024-11-18T18:21:40.283194Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n    BitsAndBytesConfig\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport torch\nfrom huggingface_hub import login","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:40.285016Z","iopub.execute_input":"2024-11-18T18:21:40.285388Z","iopub.status.idle":"2024-11-18T18:21:54.397340Z","shell.execute_reply.started":"2024-11-18T18:21:40.285357Z","shell.execute_reply":"2024-11-18T18:21:54.396643Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Print GPU info\n!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:54.399607Z","iopub.execute_input":"2024-11-18T18:21:54.400151Z","iopub.status.idle":"2024-11-18T18:21:55.543100Z","shell.execute_reply.started":"2024-11-18T18:21:54.400121Z","shell.execute_reply":"2024-11-18T18:21:55.541976Z"}},"outputs":[{"name":"stdout","text":"Mon Nov 18 18:21:55 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   54C    P8             12W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   53C    P8             12W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Clear any existing memory\nimport gc\nimport torch\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:55.544427Z","iopub.execute_input":"2024-11-18T18:21:55.544754Z","iopub.status.idle":"2024-11-18T18:21:56.007009Z","shell.execute_reply.started":"2024-11-18T18:21:55.544725Z","shell.execute_reply":"2024-11-18T18:21:56.006228Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Load JSONL file\ndata = []\nwith open('combined_dataset.json', 'r') as f:\n    for line in f:\n        try:\n            data.append(json.loads(line.strip()))\n        except json.JSONDecodeError:\n            continue\n\n# Convert to pandas DataFrame\ndf = pd.DataFrame(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:56.008476Z","iopub.execute_input":"2024-11-18T18:21:56.008812Z","iopub.status.idle":"2024-11-18T18:21:56.052665Z","shell.execute_reply.started":"2024-11-18T18:21:56.008753Z","shell.execute_reply":"2024-11-18T18:21:56.051834Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Print sample to verify data\nprint(\"Sample data:\")\nprint(df.head())\nprint(\"\\nColumns:\", df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:56.053848Z","iopub.execute_input":"2024-11-18T18:21:56.054202Z","iopub.status.idle":"2024-11-18T18:21:56.065270Z","shell.execute_reply.started":"2024-11-18T18:21:56.054162Z","shell.execute_reply":"2024-11-18T18:21:56.064336Z"}},"outputs":[{"name":"stdout","text":"Sample data:\n                                             Context  \\\n0  I'm going through some things with my feelings...   \n1  I'm going through some things with my feelings...   \n2  I'm going through some things with my feelings...   \n3  I'm going through some things with my feelings...   \n4  I'm going through some things with my feelings...   \n\n                                            Response  \n0  If everyone thinks you're worthless, then mayb...  \n1  Hello, and thank you for your question and see...  \n2  First thing I'd suggest is getting the sleep y...  \n3  Therapy is essential for those that are feelin...  \n4  I first want to let you know that you are not ...  \n\nColumns: ['Context', 'Response']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Split into train/val\ntrain_df = df.sample(frac=0.8, random_state=42)\nval_df = df.drop(train_df.index)\n\n# Convert to HF datasets\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:56.066283Z","iopub.execute_input":"2024-11-18T18:21:56.066546Z","iopub.status.idle":"2024-11-18T18:21:56.140775Z","shell.execute_reply.started":"2024-11-18T18:21:56.066512Z","shell.execute_reply":"2024-11-18T18:21:56.140098Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"print(f\"\\nTraining examples: {len(train_dataset)}\")\nprint(f\"Validation examples: {len(val_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:56.141890Z","iopub.execute_input":"2024-11-18T18:21:56.142154Z","iopub.status.idle":"2024-11-18T18:21:56.147190Z","shell.execute_reply.started":"2024-11-18T18:21:56.142127Z","shell.execute_reply":"2024-11-18T18:21:56.146215Z"}},"outputs":[{"name":"stdout","text":"\nTraining examples: 2810\nValidation examples: 702\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Set your Hugging Face token\nHF_TOKEN = \"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\"  # Replace with your token\nlogin(token=HF_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:56.148195Z","iopub.execute_input":"2024-11-18T18:21:56.148464Z","iopub.status.idle":"2024-11-18T18:21:56.264013Z","shell.execute_reply.started":"2024-11-18T18:21:56.148438Z","shell.execute_reply":"2024-11-18T18:21:56.263251Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Format conversations\ndef format_conversation(example):\n    return {\n        'text': f\"User: {example['Context']}\\nAssistant: {example['Response']}\"\n    }\n\ntrain_dataset = train_dataset.map(format_conversation)\nval_dataset = val_dataset.map(format_conversation)\n\nprint(\"\\nSample formatted conversation:\")\nprint(train_dataset[0]['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:56.265130Z","iopub.execute_input":"2024-11-18T18:21:56.265960Z","iopub.status.idle":"2024-11-18T18:21:56.523202Z","shell.execute_reply.started":"2024-11-18T18:21:56.265917Z","shell.execute_reply":"2024-11-18T18:21:56.522355Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2810 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e2e80cbe02a413a9751c8c6e37826ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/702 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fefd6dc85d846689f152f1314e3cb25"}},"metadata":{}},{"name":"stdout","text":"\nSample formatted conversation:\nUser: I've hit my head on walls and floors ever since I was young. I sometimes still do it but I don't exactly know why,    I have anxiety and I had a rough childhood but now I'll start to hit my head and sometimes not realize it but I don't know how to stop or even why I'm doing it.    How can I help myself to change my behavior?\nAssistant: The best way to handle anxiety of this level is with a combination of appropriate medication given to you by a medical doctor, and therapy to help you understand the thoughts, feelings, and behaviors that are causing the anxiety. This is not something that anyone should just “white knuckle” and try to get through on their own with no help. Cognitive Behavioral Therapy is a technique that has been proven helpful for depression and anxiety. This takes a therapist trained in CBT. You will learn to recognize when and why you perform the behavior of hitting your head, help you deal with the underlying cause of this, and replace the behavior with a more positive behavior. You'll learn coping skills.You mention having a rough childhood. Anyone who has experienced trauma like this, especially long-term ongoing trauma from abuse of any kind, definitely does not need \"exposure therapy\", which is what is recommended for phobias. You need a therapist trained specifically in trauma informed therapy.You are on the right path by recognizing there is an issue and what it is. Good luck with your healing journey!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Get current device\ndevice = torch.cuda.current_device()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:56.524232Z","iopub.execute_input":"2024-11-18T18:21:56.524484Z","iopub.status.idle":"2024-11-18T18:21:56.609644Z","shell.execute_reply.started":"2024-11-18T18:21:56.524457Z","shell.execute_reply":"2024-11-18T18:21:56.609000Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Configure 4-bit quantization with maximum memory savings\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:56.612705Z","iopub.execute_input":"2024-11-18T18:21:56.612965Z","iopub.status.idle":"2024-11-18T18:21:56.618058Z","shell.execute_reply.started":"2024-11-18T18:21:56.612941Z","shell.execute_reply":"2024-11-18T18:21:56.617346Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Initialize model and tokenizer\nprint(\"\\nInitializing model and tokenizer...\")\nmodel_name = \"google/gemma-2b-it\"\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name, \n    token=HF_TOKEN,\n    trust_remote_code=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:56.619361Z","iopub.execute_input":"2024-11-18T18:21:56.619751Z","iopub.status.idle":"2024-11-18T18:21:59.092527Z","shell.execute_reply.started":"2024-11-18T18:21:56.619713Z","shell.execute_reply":"2024-11-18T18:21:59.091824Z"}},"outputs":[{"name":"stdout","text":"\nInitializing model and tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7579a0958e2844549820eeecf634ebdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfb3d72bcba44c46b67c92b66b5be492"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0736830643247009fc8b3f4dcf50464"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37b411391d7c4e318e9ea693ba25cde4"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# Model loading with different memory settings\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    token=HF_TOKEN,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n    use_cache=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:59.093632Z","iopub.execute_input":"2024-11-18T18:21:59.093936Z","iopub.status.idle":"2024-11-18T18:24:05.486203Z","shell.execute_reply.started":"2024-11-18T18:21:59.093909Z","shell.execute_reply":"2024-11-18T18:24:05.485222Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12e07dab5d1e40fdbec376fe18eccc60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2199174741784139972c4efdbf711f88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbc6e0ef9e344c16b70b0999886d993d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"523ddbb9c3a54ecebc1f946fa033d30b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19721ba909d841cea35e5a6984cd6df9"}},"metadata":{}},{"name":"stderr","text":"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\nGemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n`config.hidden_activation` if you want to override this behaviour.\nSee https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29ef82ab65c34105aa4233cd19fb913d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c750e1a4a5cd49548f341fb3d1baa599"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Prepare model for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Configure LoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Get PEFT model\nmodel = get_peft_model(model, lora_config)\nprint(\"\\nTrainable parameters:\")\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:24:05.487614Z","iopub.execute_input":"2024-11-18T18:24:05.488009Z","iopub.status.idle":"2024-11-18T18:24:05.647998Z","shell.execute_reply.started":"2024-11-18T18:24:05.487967Z","shell.execute_reply":"2024-11-18T18:24:05.647071Z"}},"outputs":[{"name":"stdout","text":"\nTrainable parameters:\ntrainable params: 3,686,400 || all params: 2,509,858,816 || trainable%: 0.1469\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Tokenize datasets\ndef tokenize(examples):\n    return tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=512\n    )\n\nprint(\"\\nTokenizing datasets...\")\ntokenized_train = train_dataset.map(tokenize, batched=True, remove_columns=train_dataset.column_names)\ntokenized_val = val_dataset.map(tokenize, batched=True, remove_columns=val_dataset.column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:24:05.649133Z","iopub.execute_input":"2024-11-18T18:24:05.649395Z","iopub.status.idle":"2024-11-18T18:24:09.446314Z","shell.execute_reply.started":"2024-11-18T18:24:05.649369Z","shell.execute_reply":"2024-11-18T18:24:09.445392Z"}},"outputs":[{"name":"stdout","text":"\nTokenizing datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2810 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88833d52867542e4ae0e5780352e434e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/702 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec651355aa574ecd8cc37bad74e893da"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# Memory optimization environment variables\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"  # Simplified memory config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:24:09.447455Z","iopub.execute_input":"2024-11-18T18:24:09.447771Z","iopub.status.idle":"2024-11-18T18:24:09.451744Z","shell.execute_reply.started":"2024-11-18T18:24:09.447725Z","shell.execute_reply":"2024-11-18T18:24:09.450838Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Training arguments - balanced optimization\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=1,        # Minimal batch size\n    per_device_eval_batch_size=1,         \n    gradient_accumulation_steps=32,       # Increased to compensate for small batch\n    warmup_steps=50,                    \n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=20,                   \n    eval_strategy=\"epoch\",              # Keep evaluation, but only per epoch\n    save_strategy=\"epoch\",             \n    load_best_model_at_end=True,       # Keep this for best model\n    gradient_checkpointing=True,\n    report_to=\"tensorboard\",           # Keep tensorboard reporting\n    remove_unused_columns=False,\n    learning_rate=3e-4,                \n    fp16=True,                         \n    max_grad_norm=0.3,                 \n    optim=\"paged_adamw_32bit\",\n    lr_scheduler_type=\"cosine\",        \n    dataloader_num_workers=0,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n)\n# Additional model loading parameters\nmodel_kwargs = {\n    \"device_map\": \"auto\",\n    \"max_memory\": {0: \"10GB\"},  # Limit memory usage\n    \"torch_dtype\": torch.float16\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:24:09.452866Z","iopub.execute_input":"2024-11-18T18:24:09.453121Z","iopub.status.idle":"2024-11-18T18:24:09.494625Z","shell.execute_reply.started":"2024-11-18T18:24:09.453096Z","shell.execute_reply":"2024-11-18T18:24:09.493766Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Initialize trainer\nprint(\"\\nInitializing trainer...\")\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n)\n\n# Train with error handling\nprint(\"\\nStarting training...\")\ntry:\n    trainer.train()\nexcept Exception as e:\n    print(f\"Error during training: {str(e)}\")\n    # Free up memory\n    torch.cuda.empty_cache()\n    raise e\n\n# Save trained model\nprint(\"\\nSaving model...\")\ntrainer.model.save_pretrained(\"./final_model_lora\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:24:09.496006Z","iopub.execute_input":"2024-11-18T18:24:09.496349Z","iopub.status.idle":"2024-11-18T20:30:37.498645Z","shell.execute_reply.started":"2024-11-18T18:24:09.496309Z","shell.execute_reply":"2024-11-18T20:30:37.497947Z"}},"outputs":[{"name":"stdout","text":"\nInitializing trainer...\n\nStarting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='261' max='261' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [261/261 2:06:01, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2.489200</td>\n      <td>2.444245</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2.278400</td>\n      <td>2.308549</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.143900</td>\n      <td>2.281812</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nSaving model...\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"def generate_response(prompt, max_length=256):\n    try:\n        formatted_prompt = (\n            f\"User: {prompt}\\n\"\n            \"Assistant: I hear you, and what you're feeling is valid. You're not alone in this, and there are ways to help. \"\n            \"Let me share some supportive suggestions that might help you feel better. \"\n        )\n        \n        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True, max_length=max_length).to(model.device)\n        \n        outputs = model.generate(\n            **inputs,\n            max_length=max_length,\n            num_return_sequences=1,\n            temperature=0.6,\n            do_sample=True,\n            top_p=0.85,\n            top_k=40,\n            no_repeat_ngram_size=3,\n            repetition_penalty=1.3,\n            length_penalty=1.1\n        )\n        \n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        response = response.replace(formatted_prompt, \"\")\n        \n        # Combined list of patterns to remove\n        patterns_to_remove = [\n            # Endings/Signatures\n            \"Please contact\", \"Best,\", \"Best regards\", \"Sincerely\",\n            \"Dr.\", \"Licensed\", \"Certified\", \"Therapist\", \"Counselor\",\n            \"I hope this helps\", \"Remember,\", \"reach out\", \":)\", \"💫\",\n            \"Best wishes\", \"Take care\", \"Warm regards\", \"Contact me\",\n            \"For more information\", \"Feel free to\",\n            \n            # Assumptions/References\n            \"you mentioned\", \"you said\", \"already\", \"as we discussed\",\n            \"years in\", \"my suggestion\", \"I am\", \"my experience\",\n            \"If you are in\", \"please contact\", \"call\", \"website\",\n            \"helpline\", \"1-800\", \"1-\", \"800-\", \"www.\", \"http\"\n        ]\n        \n        for pattern in patterns_to_remove:\n            if pattern.lower() in response.lower():\n                response = response.split(pattern)[0]\n        \n        return response.strip()\n        \n    except Exception as e:\n        return f\"Error generating response: {str(e)}\"\n        \n# Test examples\ntest_prompts = [\n    \"I've been feeling really anxious lately about work.\",\n    \"I can't sleep at night because of stress.\",\n    \"I feel lonely and isolated.\"\n]\n\nprint(\"\\nTesting model with example prompts:\")\nfor prompt in test_prompts:\n    response = generate_response(prompt)\n    print(f\"\\nUser: {prompt}\")\n    print(f\"Assistant: {response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T20:30:37.499887Z","iopub.execute_input":"2024-11-18T20:30:37.500162Z","iopub.status.idle":"2024-11-18T20:31:21.011405Z","shell.execute_reply.started":"2024-11-18T20:30:37.500135Z","shell.execute_reply":"2024-11-18T20:31:21.010495Z"}},"outputs":[{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"\nTesting model with example prompts:\n\nUser: I've been feeling really anxious lately about work.\nAssistant: 1.  Find a counselor or therapist that works with your area of concern. There are many options online and in your area. 2.  Explore mindfulness practices like meditation and yoga. These practices can help you stay focused and calm. 3.  Consider joining a support group. This could be a support groups for anxiety, depression, or even workplace stress. 4.  If you have the means, consider exploring other outlets to help you relax.  5.  Be kind to yourself. Sometimes when we feel overwhelmed, we don't do anything at all. It's okay to take a break from everything. 6.  Remember that you are capable of handling things on your own. If you need help,\n\nUser: I can't sleep at night because of stress.\nAssistant: 1.  Make sure you've done the basics.  Check out the American Academy of Sleep Medicine's\n\nUser: I feel lonely and isolated.\nAssistant: 1. Look for other people who share your interests. 2. Join a local support group. 3. If you have a friend or family member who feels like they can offer support, let them know. 4. Consider joining a support group online. There are many groups available. 5. If these things don't work, consider talking to a therapist about how you'd like to connect with others. 6.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# save both model and tokenizer\noutput_dir = \"./supportive-ai-model\"\n\n# Save model\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\nprint(f\"Model and tokenizer saved to {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T20:31:21.012498Z","iopub.execute_input":"2024-11-18T20:31:21.012765Z","iopub.status.idle":"2024-11-18T20:31:21.748577Z","shell.execute_reply.started":"2024-11-18T20:31:21.012735Z","shell.execute_reply":"2024-11-18T20:31:21.747666Z"}},"outputs":[{"name":"stdout","text":"Model and tokenizer saved to ./supportive-ai-model\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"!pip install gradio --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T20:31:51.770011Z","iopub.execute_input":"2024-11-18T20:31:51.770646Z","iopub.status.idle":"2024-11-18T20:32:00.353378Z","shell.execute_reply.started":"2024-11-18T20:31:51.770606Z","shell.execute_reply":"2024-11-18T20:32:00.351828Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}