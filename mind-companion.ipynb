{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/gouthamvarmaindukuri/mind-companion?scriptVersionId=208259590\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Install dependencies\n!pip install -q transformers accelerate bitsandbytes peft\n!pip install -q huggingface_hub","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:14.336331Z","iopub.execute_input":"2024-11-18T18:21:14.336665Z","iopub.status.idle":"2024-11-18T18:21:36.063478Z","shell.execute_reply.started":"2024-11-18T18:21:14.336629Z","shell.execute_reply":"2024-11-18T18:21:36.062494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Download dataset directly\n!wget https://huggingface.co/datasets/Amod/mental_health_counseling_conversations/resolve/main/combined_dataset.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:36.065353Z","iopub.execute_input":"2024-11-18T18:21:36.065644Z","iopub.status.idle":"2024-11-18T18:21:37.511535Z","shell.execute_reply.started":"2024-11-18T18:21:36.065615Z","shell.execute_reply":"2024-11-18T18:21:37.510695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add at the start of your code\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Set environment variables\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"JAX_DISABLE_FORK\"] = \"1\"\n\n# Update torch amp settings\nimport torch\ntorch.amp.GradScaler = lambda *args, **kwargs: torch.amp.GradScaler(\"cuda\", *args, **kwargs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:37.512746Z","iopub.execute_input":"2024-11-18T18:21:37.51305Z","iopub.status.idle":"2024-11-18T18:21:40.283946Z","shell.execute_reply.started":"2024-11-18T18:21:37.51302Z","shell.execute_reply":"2024-11-18T18:21:40.283194Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n    BitsAndBytesConfig\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport torch\nfrom huggingface_hub import login","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:40.285016Z","iopub.execute_input":"2024-11-18T18:21:40.285388Z","iopub.status.idle":"2024-11-18T18:21:54.39734Z","shell.execute_reply.started":"2024-11-18T18:21:40.285357Z","shell.execute_reply":"2024-11-18T18:21:54.396643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print GPU info\n!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:54.399607Z","iopub.execute_input":"2024-11-18T18:21:54.400151Z","iopub.status.idle":"2024-11-18T18:21:55.5431Z","shell.execute_reply.started":"2024-11-18T18:21:54.400121Z","shell.execute_reply":"2024-11-18T18:21:55.541976Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clear any existing memory\nimport gc\nimport torch\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:55.544427Z","iopub.execute_input":"2024-11-18T18:21:55.544754Z","iopub.status.idle":"2024-11-18T18:21:56.007009Z","shell.execute_reply.started":"2024-11-18T18:21:55.544725Z","shell.execute_reply":"2024-11-18T18:21:56.006228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load JSONL file\ndata = []\nwith open('combined_dataset.json', 'r') as f:\n    for line in f:\n        try:\n            data.append(json.loads(line.strip()))\n        except json.JSONDecodeError:\n            continue\n\n# Convert to pandas DataFrame\ndf = pd.DataFrame(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:56.008476Z","iopub.execute_input":"2024-11-18T18:21:56.008812Z","iopub.status.idle":"2024-11-18T18:21:56.052665Z","shell.execute_reply.started":"2024-11-18T18:21:56.008753Z","shell.execute_reply":"2024-11-18T18:21:56.051834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print sample to verify data\nprint(\"Sample data:\")\nprint(df.head())\nprint(\"\\nColumns:\", df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:56.053848Z","iopub.execute_input":"2024-11-18T18:21:56.054202Z","iopub.status.idle":"2024-11-18T18:21:56.06527Z","shell.execute_reply.started":"2024-11-18T18:21:56.054162Z","shell.execute_reply":"2024-11-18T18:21:56.064336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split into train/val\ntrain_df = df.sample(frac=0.8, random_state=42)\nval_df = df.drop(train_df.index)\n\n# Convert to HF datasets\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:56.066283Z","iopub.execute_input":"2024-11-18T18:21:56.066546Z","iopub.status.idle":"2024-11-18T18:21:56.140775Z","shell.execute_reply.started":"2024-11-18T18:21:56.066512Z","shell.execute_reply":"2024-11-18T18:21:56.140098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"\\nTraining examples: {len(train_dataset)}\")\nprint(f\"Validation examples: {len(val_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:56.14189Z","iopub.execute_input":"2024-11-18T18:21:56.142154Z","iopub.status.idle":"2024-11-18T18:21:56.14719Z","shell.execute_reply.started":"2024-11-18T18:21:56.142127Z","shell.execute_reply":"2024-11-18T18:21:56.146215Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set your Hugging Face token\nHF_TOKEN = \"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\"  # Replace with your token\nlogin(token=HF_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:56.148195Z","iopub.execute_input":"2024-11-18T18:21:56.148464Z","iopub.status.idle":"2024-11-18T18:21:56.264013Z","shell.execute_reply.started":"2024-11-18T18:21:56.148438Z","shell.execute_reply":"2024-11-18T18:21:56.263251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Format conversations\ndef format_conversation(example):\n    return {\n        'text': f\"User: {example['Context']}\\nAssistant: {example['Response']}\"\n    }\n\ntrain_dataset = train_dataset.map(format_conversation)\nval_dataset = val_dataset.map(format_conversation)\n\nprint(\"\\nSample formatted conversation:\")\nprint(train_dataset[0]['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:56.26513Z","iopub.execute_input":"2024-11-18T18:21:56.26596Z","iopub.status.idle":"2024-11-18T18:21:56.523202Z","shell.execute_reply.started":"2024-11-18T18:21:56.265917Z","shell.execute_reply":"2024-11-18T18:21:56.522355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get current device\ndevice = torch.cuda.current_device()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:56.524232Z","iopub.execute_input":"2024-11-18T18:21:56.524484Z","iopub.status.idle":"2024-11-18T18:21:56.609644Z","shell.execute_reply.started":"2024-11-18T18:21:56.524457Z","shell.execute_reply":"2024-11-18T18:21:56.609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configure 4-bit quantization with maximum memory savings\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:56.612705Z","iopub.execute_input":"2024-11-18T18:21:56.612965Z","iopub.status.idle":"2024-11-18T18:21:56.618058Z","shell.execute_reply.started":"2024-11-18T18:21:56.612941Z","shell.execute_reply":"2024-11-18T18:21:56.617346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize model and tokenizer\nprint(\"\\nInitializing model and tokenizer...\")\nmodel_name = \"google/gemma-2b-it\"\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name, \n    token=HF_TOKEN,\n    trust_remote_code=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:56.619361Z","iopub.execute_input":"2024-11-18T18:21:56.619751Z","iopub.status.idle":"2024-11-18T18:21:59.092527Z","shell.execute_reply.started":"2024-11-18T18:21:56.619713Z","shell.execute_reply":"2024-11-18T18:21:59.091824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model loading with different memory settings\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    token=HF_TOKEN,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n    use_cache=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:21:59.093632Z","iopub.execute_input":"2024-11-18T18:21:59.093936Z","iopub.status.idle":"2024-11-18T18:24:05.486203Z","shell.execute_reply.started":"2024-11-18T18:21:59.093909Z","shell.execute_reply":"2024-11-18T18:24:05.485222Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare model for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Configure LoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Get PEFT model\nmodel = get_peft_model(model, lora_config)\nprint(\"\\nTrainable parameters:\")\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:24:05.487614Z","iopub.execute_input":"2024-11-18T18:24:05.488009Z","iopub.status.idle":"2024-11-18T18:24:05.647998Z","shell.execute_reply.started":"2024-11-18T18:24:05.487967Z","shell.execute_reply":"2024-11-18T18:24:05.647071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenize datasets\ndef tokenize(examples):\n    return tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=512\n    )\n\nprint(\"\\nTokenizing datasets...\")\ntokenized_train = train_dataset.map(tokenize, batched=True, remove_columns=train_dataset.column_names)\ntokenized_val = val_dataset.map(tokenize, batched=True, remove_columns=val_dataset.column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:24:05.649133Z","iopub.execute_input":"2024-11-18T18:24:05.649395Z","iopub.status.idle":"2024-11-18T18:24:09.446314Z","shell.execute_reply.started":"2024-11-18T18:24:05.649369Z","shell.execute_reply":"2024-11-18T18:24:09.445392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Memory optimization environment variables\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"  # Simplified memory config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:24:09.447455Z","iopub.execute_input":"2024-11-18T18:24:09.447771Z","iopub.status.idle":"2024-11-18T18:24:09.451744Z","shell.execute_reply.started":"2024-11-18T18:24:09.447725Z","shell.execute_reply":"2024-11-18T18:24:09.450838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training arguments - balanced optimization\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=1,        # Minimal batch size\n    per_device_eval_batch_size=1,         \n    gradient_accumulation_steps=32,       # Increased to compensate for small batch\n    warmup_steps=50,                    \n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=20,                   \n    eval_strategy=\"epoch\",              # Keep evaluation, but only per epoch\n    save_strategy=\"epoch\",             \n    load_best_model_at_end=True,       # Keep this for best model\n    gradient_checkpointing=True,\n    report_to=\"tensorboard\",           # Keep tensorboard reporting\n    remove_unused_columns=False,\n    learning_rate=3e-4,                \n    fp16=True,                         \n    max_grad_norm=0.3,                 \n    optim=\"paged_adamw_32bit\",\n    lr_scheduler_type=\"cosine\",        \n    dataloader_num_workers=0,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n)\n# Additional model loading parameters\nmodel_kwargs = {\n    \"device_map\": \"auto\",\n    \"max_memory\": {0: \"10GB\"},  # Limit memory usage\n    \"torch_dtype\": torch.float16\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:24:09.452866Z","iopub.execute_input":"2024-11-18T18:24:09.453121Z","iopub.status.idle":"2024-11-18T18:24:09.494625Z","shell.execute_reply.started":"2024-11-18T18:24:09.453096Z","shell.execute_reply":"2024-11-18T18:24:09.493766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize trainer\nprint(\"\\nInitializing trainer...\")\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n)\n\n# Train with error handling\nprint(\"\\nStarting training...\")\ntry:\n    trainer.train()\nexcept Exception as e:\n    print(f\"Error during training: {str(e)}\")\n    # Free up memory\n    torch.cuda.empty_cache()\n    raise e\n\n# Save trained model\nprint(\"\\nSaving model...\")\ntrainer.model.save_pretrained(\"./final_model_lora\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T18:24:09.496006Z","iopub.execute_input":"2024-11-18T18:24:09.496349Z","iopub.status.idle":"2024-11-18T20:30:37.498645Z","shell.execute_reply.started":"2024-11-18T18:24:09.496309Z","shell.execute_reply":"2024-11-18T20:30:37.497947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_response(prompt, max_length=256):\n    try:\n        formatted_prompt = (\n            f\"User: {prompt}\\n\"\n            \"Assistant: I hear you, and what you're feeling is valid. You're not alone in this, and there are ways to help. \"\n            \"Let me share some supportive suggestions that might help you feel better. \"\n        )\n        \n        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True, max_length=max_length).to(model.device)\n        \n        outputs = model.generate(\n            **inputs,\n            max_length=max_length,\n            num_return_sequences=1,\n            temperature=0.6,\n            do_sample=True,\n            top_p=0.85,\n            top_k=40,\n            no_repeat_ngram_size=3,\n            repetition_penalty=1.3,\n            length_penalty=1.1\n        )\n        \n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        response = response.replace(formatted_prompt, \"\")\n        \n        # Combined list of patterns to remove\n        patterns_to_remove = [\n            # Endings/Signatures\n            \"Please contact\", \"Best,\", \"Best regards\", \"Sincerely\",\n            \"Dr.\", \"Licensed\", \"Certified\", \"Therapist\", \"Counselor\",\n            \"I hope this helps\", \"Remember,\", \"reach out\", \":)\", \"💫\",\n            \"Best wishes\", \"Take care\", \"Warm regards\", \"Contact me\",\n            \"For more information\", \"Feel free to\",\n            \n            # Assumptions/References\n            \"you mentioned\", \"you said\", \"already\", \"as we discussed\",\n            \"years in\", \"my suggestion\", \"I am\", \"my experience\",\n            \"If you are in\", \"please contact\", \"call\", \"website\",\n            \"helpline\", \"1-800\", \"1-\", \"800-\", \"www.\", \"http\"\n        ]\n        \n        for pattern in patterns_to_remove:\n            if pattern.lower() in response.lower():\n                response = response.split(pattern)[0]\n        \n        return response.strip()\n        \n    except Exception as e:\n        return f\"Error generating response: {str(e)}\"\n        \n# Test examples\ntest_prompts = [\n    \"I've been feeling really anxious lately about work.\",\n    \"I can't sleep at night because of stress.\",\n    \"I feel lonely and isolated.\"\n]\n\nprint(\"\\nTesting model with example prompts:\")\nfor prompt in test_prompts:\n    response = generate_response(prompt)\n    print(f\"\\nUser: {prompt}\")\n    print(f\"Assistant: {response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T20:30:37.499887Z","iopub.execute_input":"2024-11-18T20:30:37.500162Z","iopub.status.idle":"2024-11-18T20:31:21.011405Z","shell.execute_reply.started":"2024-11-18T20:30:37.500135Z","shell.execute_reply":"2024-11-18T20:31:21.010495Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# save both model and tokenizer\noutput_dir = \"./supportive-ai-model\"\n\n# Save model\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\nprint(f\"Model and tokenizer saved to {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T20:31:21.012498Z","iopub.execute_input":"2024-11-18T20:31:21.012765Z","iopub.status.idle":"2024-11-18T20:31:21.748577Z","shell.execute_reply.started":"2024-11-18T20:31:21.012735Z","shell.execute_reply":"2024-11-18T20:31:21.747666Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install gradio --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T20:31:51.770011Z","iopub.execute_input":"2024-11-18T20:31:51.770646Z","iopub.status.idle":"2024-11-18T20:32:00.353378Z","shell.execute_reply.started":"2024-11-18T20:31:51.770606Z","shell.execute_reply":"2024-11-18T20:32:00.351828Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Load the saved model and tokenizer\nmodel_path = \"./supportive-ai-model\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    trust_remote_code=True\n)\n\ndef chat_response(message, history):\n    # Format the prompt similar to training data\n    formatted_prompt = f\"User: {message}\\nAssistant: \"\n    \n    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    \n    # Generate response\n    outputs = model.generate(\n        **inputs,\n        max_length=512,\n        num_return_sequences=1,\n        temperature=0.7,\n        do_sample=True,\n        top_p=0.85,\n        top_k=40,\n        no_repeat_ngram_size=3,\n        repetition_penalty=1.3,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    # Extract only the assistant's response\n    response = response.split(\"Assistant: \")[-1].strip()\n    \n    return response\n\n# Create Gradio Interface\ndemo = gr.ChatInterface(\n    fn=chat_response,\n    title=\"Mental Health Support Assistant\",\n    description=\"A supportive AI assistant trained to provide empathetic responses to mental health concerns. Please note: This is not a replacement for professional mental health support.\",\n    theme=\"soft\",\n    examples=[\n        \"I've been feeling really anxious lately about work.\",\n        \"I can't sleep at night because of stress.\",\n        \"I feel lonely and isolated.\"\n    ]\n)\n\n# Launch the interface\nif __name__ == \"__main__\":\n    demo.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T20:37:08.841503Z","iopub.execute_input":"2024-11-18T20:37:08.842517Z","iopub.status.idle":"2024-11-18T20:37:18.190746Z","shell.execute_reply.started":"2024-11-18T20:37:08.842465Z","shell.execute_reply":"2024-11-18T20:37:18.189996Z"}},"outputs":[],"execution_count":null}]}