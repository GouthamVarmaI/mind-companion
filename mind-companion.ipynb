{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/gouthamvarmaindukuri/mind-companion?scriptVersionId=208276006\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Install dependencies\n!pip install -q transformers accelerate bitsandbytes peft\n!pip install -q huggingface_hub","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:30:25.39142Z","iopub.execute_input":"2024-11-18T22:30:25.391834Z","iopub.status.idle":"2024-11-18T22:30:42.069897Z","shell.execute_reply.started":"2024-11-18T22:30:25.391792Z","shell.execute_reply":"2024-11-18T22:30:42.068608Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Download dataset directly\n!wget https://huggingface.co/datasets/Amod/mental_health_counseling_conversations/resolve/main/combined_dataset.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:30:42.073527Z","iopub.execute_input":"2024-11-18T22:30:42.07383Z","iopub.status.idle":"2024-11-18T22:30:43.530329Z","shell.execute_reply.started":"2024-11-18T22:30:42.073802Z","shell.execute_reply":"2024-11-18T22:30:43.529182Z"}},"outputs":[{"name":"stdout","text":"--2024-11-18 22:30:43--  https://huggingface.co/datasets/Amod/mental_health_counseling_conversations/resolve/main/combined_dataset.json\nResolving huggingface.co (huggingface.co)... 3.165.160.59, 3.165.160.11, 3.165.160.61, ...\nConnecting to huggingface.co (huggingface.co)|3.165.160.59|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4790520 (4.6M) [text/plain]\nSaving to: 'combined_dataset.json.1'\n\ncombined_dataset.js 100%[===================>]   4.57M  17.5MB/s    in 0.3s    \n\n2024-11-18 22:30:43 (17.5 MB/s) - 'combined_dataset.json.1' saved [4790520/4790520]\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Add at the start of your code\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Set environment variables\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"JAX_DISABLE_FORK\"] = \"1\"\n\n# Update torch amp settings\nimport torch\ntorch.amp.GradScaler = lambda *args, **kwargs: torch.amp.GradScaler(\"cuda\", *args, **kwargs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:30:43.531705Z","iopub.execute_input":"2024-11-18T22:30:43.532014Z","iopub.status.idle":"2024-11-18T22:30:43.537412Z","shell.execute_reply.started":"2024-11-18T22:30:43.531986Z","shell.execute_reply":"2024-11-18T22:30:43.536534Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n    BitsAndBytesConfig\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport torch\nfrom huggingface_hub import login","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:30:08.781023Z","iopub.execute_input":"2024-11-18T22:30:08.781375Z","iopub.status.idle":"2024-11-18T22:30:23.792079Z","shell.execute_reply.started":"2024-11-18T22:30:08.78135Z","shell.execute_reply":"2024-11-18T22:30:23.7914Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Print GPU info\n!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:30:23.793132Z","iopub.execute_input":"2024-11-18T22:30:23.793692Z","iopub.status.idle":"2024-11-18T22:30:24.93528Z","shell.execute_reply.started":"2024-11-18T22:30:23.793665Z","shell.execute_reply":"2024-11-18T22:30:24.934192Z"}},"outputs":[{"name":"stdout","text":"Mon Nov 18 22:30:24 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   34C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   34C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Clear any existing memory\nimport gc\nimport torch\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:30:24.937087Z","iopub.execute_input":"2024-11-18T22:30:24.937817Z","iopub.status.idle":"2024-11-18T22:30:25.312968Z","shell.execute_reply.started":"2024-11-18T22:30:24.937775Z","shell.execute_reply":"2024-11-18T22:30:25.311732Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Load JSONL file\ndata = []\nwith open('combined_dataset.json', 'r') as f:\n    for line in f:\n        try:\n            data.append(json.loads(line.strip()))\n        except json.JSONDecodeError:\n            continue\n\n# Convert to pandas DataFrame\ndf = pd.DataFrame(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:30:25.314153Z","iopub.execute_input":"2024-11-18T22:30:25.314477Z","iopub.status.idle":"2024-11-18T22:30:25.373951Z","shell.execute_reply.started":"2024-11-18T22:30:25.31445Z","shell.execute_reply":"2024-11-18T22:30:25.372965Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Print sample to verify data\nprint(\"Sample data:\")\nprint(df.head())\nprint(\"\\nColumns:\", df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:30:25.375093Z","iopub.execute_input":"2024-11-18T22:30:25.375393Z","iopub.status.idle":"2024-11-18T22:30:25.390131Z","shell.execute_reply.started":"2024-11-18T22:30:25.375366Z","shell.execute_reply":"2024-11-18T22:30:25.388902Z"}},"outputs":[{"name":"stdout","text":"Sample data:\n                                             Context  \\\n0  I'm going through some things with my feelings...   \n1  I'm going through some things with my feelings...   \n2  I'm going through some things with my feelings...   \n3  I'm going through some things with my feelings...   \n4  I'm going through some things with my feelings...   \n\n                                            Response  \n0  If everyone thinks you're worthless, then mayb...  \n1  Hello, and thank you for your question and see...  \n2  First thing I'd suggest is getting the sleep y...  \n3  Therapy is essential for those that are feelin...  \n4  I first want to let you know that you are not ...  \n\nColumns: ['Context', 'Response']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Split into train/val\ntrain_df = df.sample(frac=0.8, random_state=42)\nval_df = df.drop(train_df.index)\n\n# Convert to HF datasets\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:30:43.538733Z","iopub.execute_input":"2024-11-18T22:30:43.539074Z","iopub.status.idle":"2024-11-18T22:30:43.635331Z","shell.execute_reply.started":"2024-11-18T22:30:43.539038Z","shell.execute_reply":"2024-11-18T22:30:43.634658Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"print(f\"\\nTraining examples: {len(train_dataset)}\")\nprint(f\"Validation examples: {len(val_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:30:43.636368Z","iopub.execute_input":"2024-11-18T22:30:43.636625Z","iopub.status.idle":"2024-11-18T22:30:43.641355Z","shell.execute_reply.started":"2024-11-18T22:30:43.636601Z","shell.execute_reply":"2024-11-18T22:30:43.640506Z"}},"outputs":[{"name":"stdout","text":"\nTraining examples: 2810\nValidation examples: 702\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Set your Hugging Face token\nHF_TOKEN = \"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\"  # Replace with your token\nlogin(token=HF_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:30:43.642508Z","iopub.execute_input":"2024-11-18T22:30:43.643103Z","iopub.status.idle":"2024-11-18T22:30:43.761667Z","shell.execute_reply.started":"2024-11-18T22:30:43.643076Z","shell.execute_reply":"2024-11-18T22:30:43.760812Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Format conversations\ndef format_conversation(example):\n    return {\n        'text': f\"User: {example['Context']}\\nAssistant: {example['Response']}\"\n    }\n\ntrain_dataset = train_dataset.map(format_conversation)\nval_dataset = val_dataset.map(format_conversation)\n\nprint(\"\\nSample formatted conversation:\")\nprint(train_dataset[0]['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:30:43.762639Z","iopub.execute_input":"2024-11-18T22:30:43.762905Z","iopub.status.idle":"2024-11-18T22:30:44.026476Z","shell.execute_reply.started":"2024-11-18T22:30:43.762875Z","shell.execute_reply":"2024-11-18T22:30:44.025651Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2810 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4ab7775671f4bcb8a4ea8483a8fe780"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/702 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"651f05330a0f47f5860429298ece33fc"}},"metadata":{}},{"name":"stdout","text":"\nSample formatted conversation:\nUser: I've hit my head on walls and floors ever since I was young. I sometimes still do it but I don't exactly know why,    I have anxiety and I had a rough childhood but now I'll start to hit my head and sometimes not realize it but I don't know how to stop or even why I'm doing it.    How can I help myself to change my behavior?\nAssistant: The best way to handle anxiety of this level is with a combination of appropriate medication given to you by a medical doctor, and therapy to help you understand the thoughts, feelings, and behaviors that are causing the anxiety. This is not something that anyone should just “white knuckle” and try to get through on their own with no help. Cognitive Behavioral Therapy is a technique that has been proven helpful for depression and anxiety. This takes a therapist trained in CBT. You will learn to recognize when and why you perform the behavior of hitting your head, help you deal with the underlying cause of this, and replace the behavior with a more positive behavior. You'll learn coping skills.You mention having a rough childhood. Anyone who has experienced trauma like this, especially long-term ongoing trauma from abuse of any kind, definitely does not need \"exposure therapy\", which is what is recommended for phobias. You need a therapist trained specifically in trauma informed therapy.You are on the right path by recognizing there is an issue and what it is. Good luck with your healing journey!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Get current device\ndevice = torch.cuda.current_device()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:30:44.0278Z","iopub.execute_input":"2024-11-18T22:30:44.028228Z","iopub.status.idle":"2024-11-18T22:30:44.115039Z","shell.execute_reply.started":"2024-11-18T22:30:44.028182Z","shell.execute_reply":"2024-11-18T22:30:44.114341Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Configure 4-bit quantization with maximum memory savings\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:30:44.116038Z","iopub.execute_input":"2024-11-18T22:30:44.116295Z","iopub.status.idle":"2024-11-18T22:30:44.122262Z","shell.execute_reply.started":"2024-11-18T22:30:44.116269Z","shell.execute_reply":"2024-11-18T22:30:44.121515Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Initialize model and tokenizer\nprint(\"\\nInitializing model and tokenizer...\")\nmodel_name = \"google/gemma-2b-it\"\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name, \n    token=HF_TOKEN,\n    trust_remote_code=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:30:44.123316Z","iopub.execute_input":"2024-11-18T22:30:44.12357Z","iopub.status.idle":"2024-11-18T22:30:46.557378Z","shell.execute_reply.started":"2024-11-18T22:30:44.123547Z","shell.execute_reply":"2024-11-18T22:30:46.556419Z"}},"outputs":[{"name":"stdout","text":"\nInitializing model and tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2ba45b072d94fa085b4c182067414b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b87090a3ff8d475587db1bbdeee619c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b76df0906a44459b61b50881cd7a0ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8702e50cfb8b491db4584b38b73d7269"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# Model loading with different memory settings\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    token=HF_TOKEN,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n    use_cache=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:30:46.55876Z","iopub.execute_input":"2024-11-18T22:30:46.559124Z","iopub.status.idle":"2024-11-18T22:32:53.571998Z","shell.execute_reply.started":"2024-11-18T22:30:46.559097Z","shell.execute_reply":"2024-11-18T22:32:53.571331Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8611fe863be24f6e8b8f0cd344ecc182"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ae91b13cb44476996c6e42f8c2d53fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c16e00887de64274a5a17442bf21c7bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dff04f2d31664c6d81a0465a7db3d2de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fea485ff6a71410c8bd7d032dcff0556"}},"metadata":{}},{"name":"stderr","text":"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\nGemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n`config.hidden_activation` if you want to override this behaviour.\nSee https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"196a3e7a2d7c4ec6b9e50519e33f84ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c2ac1e638e54847aa62cdf1fa8803e8"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# Prepare model for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Configure LoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Get PEFT model\nmodel = get_peft_model(model, lora_config)\nprint(\"\\nTrainable parameters:\")\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:32:53.5731Z","iopub.execute_input":"2024-11-18T22:32:53.57338Z","iopub.status.idle":"2024-11-18T22:32:53.727194Z","shell.execute_reply.started":"2024-11-18T22:32:53.573356Z","shell.execute_reply":"2024-11-18T22:32:53.726373Z"}},"outputs":[{"name":"stdout","text":"\nTrainable parameters:\ntrainable params: 3,686,400 || all params: 2,509,858,816 || trainable%: 0.1469\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Tokenize datasets\ndef tokenize(examples):\n    return tokenizer(\n        examples[\"text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=512\n    )\n\nprint(\"\\nTokenizing datasets...\")\ntokenized_train = train_dataset.map(tokenize, batched=True, remove_columns=train_dataset.column_names)\ntokenized_val = val_dataset.map(tokenize, batched=True, remove_columns=val_dataset.column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:32:53.728231Z","iopub.execute_input":"2024-11-18T22:32:53.728465Z","iopub.status.idle":"2024-11-18T22:32:57.143891Z","shell.execute_reply.started":"2024-11-18T22:32:53.728443Z","shell.execute_reply":"2024-11-18T22:32:57.143026Z"}},"outputs":[{"name":"stdout","text":"\nTokenizing datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2810 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3db47cf567e943cf8c4a67de10f92e83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/702 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b51f1424bf14571a6b66a9283e76ea7"}},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# Memory optimization environment variables\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"  # Simplified memory config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:32:57.144903Z","iopub.execute_input":"2024-11-18T22:32:57.14515Z","iopub.status.idle":"2024-11-18T22:32:57.149323Z","shell.execute_reply.started":"2024-11-18T22:32:57.145126Z","shell.execute_reply":"2024-11-18T22:32:57.148439Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Training arguments - balanced optimization\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=1,        # Minimal batch size\n    per_device_eval_batch_size=1,         \n    gradient_accumulation_steps=32,       # Increased to compensate for small batch\n    warmup_steps=50,                    \n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=20,                   \n    eval_strategy=\"epoch\",              # Keep evaluation, but only per epoch\n    save_strategy=\"epoch\",             \n    load_best_model_at_end=True,       # Keep this for best model\n    gradient_checkpointing=True,\n    report_to=\"tensorboard\",           # Keep tensorboard reporting\n    remove_unused_columns=False,\n    learning_rate=3e-4,                \n    fp16=True,                         \n    max_grad_norm=0.3,                 \n    optim=\"paged_adamw_32bit\",\n    lr_scheduler_type=\"cosine\",        \n    dataloader_num_workers=0,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n)\n# Additional model loading parameters\nmodel_kwargs = {\n    \"device_map\": \"auto\",\n    \"max_memory\": {0: \"10GB\"},  # Limit memory usage\n    \"torch_dtype\": torch.float16\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:32:57.150529Z","iopub.execute_input":"2024-11-18T22:32:57.150869Z","iopub.status.idle":"2024-11-18T22:32:57.189938Z","shell.execute_reply.started":"2024-11-18T22:32:57.15082Z","shell.execute_reply":"2024-11-18T22:32:57.189019Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Initialize trainer\nprint(\"\\nInitializing trainer...\")\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n)\n\n# Train with error handling\nprint(\"\\nStarting training...\")\ntry:\n    trainer.train()\nexcept Exception as e:\n    print(f\"Error during training: {str(e)}\")\n    # Free up memory\n    torch.cuda.empty_cache()\n    raise e\n\n# Save trained model\nprint(\"\\nSaving model...\")\ntrainer.model.save_pretrained(\"./final_model_lora\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T22:32:57.191015Z","iopub.execute_input":"2024-11-18T22:32:57.191254Z","iopub.status.idle":"2024-11-19T00:37:55.474511Z","shell.execute_reply.started":"2024-11-18T22:32:57.19123Z","shell.execute_reply":"2024-11-19T00:37:55.473609Z"}},"outputs":[{"name":"stdout","text":"\nInitializing trainer...\n\nStarting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='261' max='261' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [261/261 2:04:32, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2.488700</td>\n      <td>2.445267</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2.278600</td>\n      <td>2.308133</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.145300</td>\n      <td>2.282104</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nSaving model...\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"def generate_response(prompt, max_length=256):\n    try:\n        formatted_prompt = (\n            f\"User: {prompt}\\n\"\n            \"Assistant: I hear you, and what you're feeling is valid. You're not alone in this, and there are ways to help. \"\n            \"Let me share some supportive suggestions that might help you feel better. \"\n        )\n        \n        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True, max_length=max_length).to(model.device)\n        \n        outputs = model.generate(\n            **inputs,\n            max_length=max_length,\n            num_return_sequences=1,\n            temperature=0.6,\n            do_sample=True,\n            top_p=0.85,\n            top_k=40,\n            no_repeat_ngram_size=3,\n            repetition_penalty=1.3,\n            length_penalty=1.1\n        )\n        \n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        response = response.replace(formatted_prompt, \"\")\n        \n        # Combined list of patterns to remove\n        patterns_to_remove = [\n            # Endings/Signatures\n            \"Please contact\", \"Best,\", \"Best regards\", \"Sincerely\",\n            \"Dr.\", \"Licensed\", \"Certified\", \"Therapist\", \"Counselor\",\n            \"I hope this helps\", \"Remember,\", \"reach out\", \":)\", \"💫\",\n            \"Best wishes\", \"Take care\", \"Warm regards\", \"Contact me\",\n            \"For more information\", \"Feel free to\",\n            \n            # Assumptions/References\n            \"you mentioned\", \"you said\", \"already\", \"as we discussed\",\n            \"years in\", \"my suggestion\", \"I am\", \"my experience\",\n            \"If you are in\", \"please contact\", \"call\", \"website\",\n            \"helpline\", \"1-800\", \"1-\", \"800-\", \"www.\", \"http\"\n        ]\n        \n        for pattern in patterns_to_remove:\n            if pattern.lower() in response.lower():\n                response = response.split(pattern)[0]\n        \n        return response.strip()\n        \n    except Exception as e:\n        return f\"Error generating response: {str(e)}\"\n        \n# Test examples\ntest_prompts = [\n    \"I've been feeling really anxious lately about work.\",\n    \"I can't sleep at night because of stress.\",\n    \"I feel lonely and isolated.\"\n]\n\nprint(\"\\nTesting model with example prompts:\")\nfor prompt in test_prompts:\n    response = generate_response(prompt)\n    print(f\"\\nUser: {prompt}\")\n    print(f\"Assistant: {response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:37:55.475811Z","iopub.execute_input":"2024-11-19T00:37:55.476171Z","iopub.status.idle":"2024-11-19T00:38:40.117251Z","shell.execute_reply.started":"2024-11-19T00:37:55.476141Z","shell.execute_reply":"2024-11-19T00:38:40.116198Z"}},"outputs":[{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"\nTesting model with example prompts:\n\nUser: I've been feeling really anxious lately about work.\nAssistant: The first thing that comes to mind is that you may be experiencing a lot of stress at work. The workplace can be very stressful, especially if you have a demanding job. If you find yourself working late or taking breaks often, it's possible that you've reached your limit and need to take a break from the workplace. If you do find yourself needing a break, try to avoid caffeine and alcohol. Both of these substances can worsen anxiety.  Another suggestion is to practice relaxation techniques. This could include yoga, meditation, or progressive muscle relaxation. These practices can help to calm your nerves and lower your heart rate.  Finally, if you'd like to talk about your concerns, I would encourage you to see a therapist. A therapist can help you to identify the source of your anxiety and develop coping mechanisms.  Best of luck! Robin, LPC/IST/RPT/CEDS/CP/BCTCP/CBT-LP/\n\nUser: I can't sleep at night because of stress.\nAssistant: Start with a simple bedtime routine. Take a walk in nature, read a book, or listen to calming music. This will help you unwind and get your mind off of the day. Next, try a relaxation technique. Deep breathing, progressive muscle relaxation, and guided imagery are all helpful. You can find guided imagery online or on apps like Headspace or Calm. Finally, consider talking with a therapist. I would suggest finding someone who specializes in cognitive behavioral therapy. CBT is a very effective way to reduce stress and anxiety, which often leads to better sleep.\n\nUser: I feel lonely and isolated.\nAssistant: 1. Look at your relationships. Are you connected with people who can support you? Are you spending time with friends and family? 2. Consider if you have any hobbies or activities that you enjoy. Are there things that you do that bring you joy? 3. Reflect on your social life. Are the times that you spend with others meaningful? Are there events that you look forward to? 4. If you find yourself having trouble connecting with others, consider talking to a counselor. Sometimes, we need external help to understand how we interact with others. 5. If loneliness is a new experience for you, it may be helpful to seek out a support group. There are many groups available for various topics, including loneliness. 6. Remember that loneliness is temporary. It's okay to feel lonely sometimes. It doesn't mean that you don't have friends and you don’t have love in your life. You just need to remind yourself that you are going to get through\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# save both model and tokenizer\noutput_dir = \"./supportive-ai-model\"\n\n# Save model\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\nprint(f\"Model and tokenizer saved to {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:38:40.11892Z","iopub.execute_input":"2024-11-19T00:38:40.119829Z","iopub.status.idle":"2024-11-19T00:38:40.949618Z","shell.execute_reply.started":"2024-11-19T00:38:40.119776Z","shell.execute_reply":"2024-11-19T00:38:40.948706Z"}},"outputs":[{"name":"stdout","text":"Model and tokenizer saved to ./supportive-ai-model\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"!pip install gradio --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:38:40.95104Z","iopub.execute_input":"2024-11-19T00:38:40.951451Z","iopub.status.idle":"2024-11-19T00:38:55.471117Z","shell.execute_reply.started":"2024-11-19T00:38:40.951407Z","shell.execute_reply":"2024-11-19T00:38:55.469828Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"import gradio as gr\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Load the saved model and tokenizer\nmodel_path = \"./supportive-ai-model\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    trust_remote_code=True\n)\n\ndef chat_response(message, history):\n    # Format the prompt similar to training data\n    formatted_prompt = f\"User: {message}\\nAssistant: \"\n    \n    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    \n    # Generate response\n    outputs = model.generate(\n        **inputs,\n        max_length=512,\n        num_return_sequences=1,\n        temperature=0.7,\n        do_sample=True,\n        top_p=0.85,\n        top_k=40,\n        no_repeat_ngram_size=3,\n        repetition_penalty=1.3,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    # Extract only the assistant's response\n    response = response.split(\"Assistant: \")[-1].strip()\n    \n    return response\n\n# Create Gradio Interface\ndemo = gr.ChatInterface(\n    fn=chat_response,\n    title=\"Mind Companion\",\n    description=\"A supportive AI assistant trained to provide empathetic responses to mental health concerns. Please note: This is not a replacement for professional mental health support.\",\n    theme=\"soft\",\n    examples=[\n        \"I've been feeling really anxious lately about work.\",\n        \"I can't sleep at night because of stress.\",\n        \"I feel lonely and isolated.\"\n    ]\n)\n\n# Launch the interface\nif __name__ == \"__main__\":\n    demo.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T00:39:45.897245Z","iopub.execute_input":"2024-11-19T00:39:45.897932Z","iopub.status.idle":"2024-11-19T00:39:54.517796Z","shell.execute_reply.started":"2024-11-19T00:39:45.897886Z","shell.execute_reply":"2024-11-19T00:39:54.516747Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21ef4df1bcc8409fad4dccaf0d9dc0aa"}},"metadata":{}},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7861\n* Running on public URL: https://c713cabe3fa36e2e7d.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://c713cabe3fa36e2e7d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":29}]}